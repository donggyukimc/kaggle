{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom kaggle.competitions import twosigmanews",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['marketdata_sample.csv', 'news_sample.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60b15f403db7502b4dc975cfe2902254077bd27e"
      },
      "cell_type": "code",
      "source": "env = twosigmanews.make_env()\n(marketdf, newsdf) = env.get_training_data()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading the data... This could take a minute.\nDone!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def prepare_data(marketdf, newsdf):\n\n    # a bit of feature engineering\n    marketdf['time'] = marketdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['time']   = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    \n    # filter pre-2012 data, no particular reason\n    marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    newsdf   = newsdf.loc[newsdf['time'] > 20120000]\n    \n    marketdf['today_trend'] = marketdf['close'] / marketdf['open']\n    marketdf['middle']      = (marketdf['close'] + marketdf['open'])/2\n    marketdf['money']       = marketdf['volume'] * marketdf['middle']\n    \n    marketdf['returnsClosePrevRaw_sum'] = marketdf['returnsClosePrevRaw1'] + marketdf['returnsClosePrevRaw10']\n    marketdf['returnsOpenPrevRaw_sum']  = marketdf['returnsOpenPrevRaw1'] + marketdf['returnsOpenPrevRaw10']\n    \n    marketdf['returnsClosePrevRaw_avg'] = (marketdf['returnsClosePrevRaw1'] + marketdf['returnsClosePrevRaw10']) / 2\n    marketdf['returnsOpenPrevRaw_avg']  = (marketdf['returnsOpenPrevRaw1'] + marketdf['returnsOpenPrevRaw10']) / 2\n    \n    marketdf['returnsCloseOpenRaw1_sum']  = marketdf['returnsClosePrevRaw1'] + marketdf['returnsOpenPrevRaw1']\n    marketdf['returnsCloseOpenRaw10_sum'] = marketdf['returnsClosePrevRaw10'] + marketdf['returnsOpenPrevRaw10']\n    \n    marketdf['returnsCloseOpenRaw1_avg']  = (marketdf['returnsClosePrevRaw1'] + marketdf['returnsOpenPrevRaw1']) / 2\n    marketdf['returnsCloseOpenRaw10_avg'] = (marketdf['returnsClosePrevRaw10'] + marketdf['returnsOpenPrevRaw10']) / 2\n    \n    marketdf['returnsClosePrevMktres_sum'] = marketdf['returnsClosePrevMktres1'] + marketdf['returnsClosePrevMktres10']\n    marketdf['returnsOpenPrevMktres_sum']  = marketdf['returnsOpenPrevMktres1'] + marketdf['returnsOpenPrevMktres10']\n    \n    marketdf['returnsClosePrevMktres_avg'] = (marketdf['returnsClosePrevMktres1'] + marketdf['returnsClosePrevMktres10']) / 2\n    marketdf['returnsOpenPrevMktres_avg']  = (marketdf['returnsOpenPrevMktres1'] + marketdf['returnsOpenPrevMktres10']) / 2\n    \n    marketdf['returnsCloseOpenMktres1_sum']  = marketdf['returnsClosePrevMktres1'] + marketdf['returnsOpenPrevMktres1']\n    marketdf['returnsCloseOpenMktres10_sum'] = marketdf['returnsClosePrevMktres10'] + marketdf['returnsOpenPrevMktres10']\n    \n    marketdf['returnsCloseOpenMktres1_avg']  = (marketdf['returnsClosePrevMktres1'] + marketdf['returnsOpenPrevMktres1']) / 2\n    marketdf['returnsCloseOpenMktres10_avg'] = (marketdf['returnsClosePrevMktres10'] + marketdf['returnsOpenPrevMktres10']) / 2\n    \n    marketdf.drop(['open', 'close', 'middle'], axis = 1, inplace = True)\n    \n    newsdf.loc[newsdf['firstMentionSentence'] == 0, 'firstMentionSentence'] = newsdf.loc[newsdf['firstMentionSentence'] == 0, 'sentenceCount']\n    \n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] / newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] / newsdf['wordCount']\n    \n    # get rid of extra junk from news data\n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence'\n                #'sentenceCount'\n                ,'bodySize','headlineTag','marketCommentary','subjects','audiences'\n                #,'sentimentClass'\n                ,'assetName', 'assetCodes'\n                #,'urgency'\n                ,'wordCount','sentimentWordCount']\n    newsdf.drop(droplist, axis=1, inplace=True)\n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    \n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).agg(['min', 'max', 'mean', 'std'])\n    newsgp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in newsgp.columns.tolist()])\n    \n    # join news reports to market data, note many assets will have many days without news data\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd4ebbefc9548a0de77c2732ce5b43d53e697d54",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print('preparing data...')\n\ncdf = prepare_data(marketdf, newsdf)    \ndel marketdf, newsdf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "preparing data...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  # This is added back by InteractiveShellApp.init_path()\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if sys.path[0] == '':\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  del sys.path[0]\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  app.launch_new_instance()\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  errors=errors)\n/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af5a4edcae6ee495622076d688e0b701bb515e52"
      },
      "cell_type": "code",
      "source": "cdf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c57925f841fd5b7c2b3a3857e82244433838dfc",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nimport gc\n\nprint('building training set...')\n\ncdf['target'] = (cdf['returnsOpenNextMktres10'] > 0.0).astype(int)\nfeatures = [col for col in cdf.columns if col not in ['time', 'assetCode', 'universe', 'returnsOpenNextMktres10', 'target']]\n\n#######################################################\n##\n## LightGBM\n##\n#######################################################\nimport lightgbm as lgb\nprint ('Training lightgbm')\n\nparams = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n          \"num_leaves\" : 60,\n          \"max_depth\": 10,\n          \"learning_rate\" : 0.005,\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 2018,\n          \"lambda_l1\" : 1.0,\n          \"verbosity\" : -1 }\n\nprint(\"using params \", params)\nprint(\"using features \", features)\nprint(\"\")\n\ndates = list(cdf['time'].unique())\n\nk_fold = True\n\nif k_fold :\n    K = 1\n    for k in range(1, K + 1) :\n\n        k_dates = dates[:int(len(dates)*k/(K))]\n\n        train_idx, val_idx = train_test_split(k_dates, test_size = 0.15, shuffle = False)\n        #print(train_idx)\n        #print(val_idx)\n\n        train_idx = cdf[cdf['time'].isin(train_idx)].index.values\n        val_idx = cdf[cdf['time'].isin(val_idx)].index.values\n\n        print(k, \"K using data from \", cdf.loc[train_idx, 'time'].min(), \" to \", cdf.loc[val_idx, 'time'].max())\n\n        train_data = lgb.Dataset(cdf.loc[train_idx, features], cdf.loc[train_idx, 'target'])\n        val_data   = lgb.Dataset(cdf.loc[val_idx, features], cdf.loc[val_idx, 'target'])\n\n        lgb.reset_parameter()\n        model = lgb.train(params, train_data, 10000, valid_sets = [train_data, val_data], early_stopping_rounds = 100, verbose_eval = 50)\n        \n        feature_imp = pd.DataFrame(sorted(zip(features, model.feature_importance(), model.feature_importance(\"gain\"))), columns=['feature', 'split', 'gain']).sort_values(by=['split'])\n        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n            print(feature_imp)\n        \n        predict = model.predict(cdf.loc[val_idx, features], num_iteration = model.best_iteration)\n        confidence_valid = predict.reshape(-1) * 2 - 1\n        x_t_i = confidence_valid * cdf.loc[val_idx, 'returnsOpenNextMktres10'] * cdf.loc[val_idx, 'universe']\n        data = {'day' : cdf.loc[val_idx, 'time'], 'x_t_i' : x_t_i}\n        df_predict = pd.DataFrame(data)\n        x_t = df_predict.groupby('day').sum().values.flatten()\n        mean = np.mean(x_t)\n        std = np.std(x_t)\n        score_valid = mean / std\n        print(\"two sigma score : \", score_valid)\n        print(\"\")\n\n        gc.collect()\nelse :\n    pass\n\n#lgtrain, lgval = lgb.Dataset(Xt, Yt[:,0]), lgb.Dataset(Xv, Yv[:,0])\n#lgbmodel = lgb.train(params, lgtrain, 2000, valid_sets=[lgtrain, lgval], early_stopping_rounds=100, verbose_eval=200)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "building training set...\nTraining lightgbm\nusing params  {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 60, 'max_depth': 10, 'learning_rate': 0.005, 'bagging_fraction': 0.9, 'feature_fraction': 0.9, 'bagging_freq': 5, 'bagging_seed': 2018, 'lambda_l1': 1.0, 'verbosity': -1}\nusing features  ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'today_trend', 'money', 'returnsClosePrevRaw_sum', 'returnsOpenPrevRaw_sum', 'returnsClosePrevRaw_avg', 'returnsOpenPrevRaw_avg', 'returnsCloseOpenRaw1_sum', 'returnsCloseOpenRaw10_sum', 'returnsCloseOpenRaw1_avg', 'returnsCloseOpenRaw10_avg', 'returnsClosePrevMktres_sum', 'returnsOpenPrevMktres_sum', 'returnsClosePrevMktres_avg', 'returnsOpenPrevMktres_avg', 'returnsCloseOpenMktres1_sum', 'returnsCloseOpenMktres10_sum', 'returnsCloseOpenMktres1_avg', 'returnsCloseOpenMktres10_avg', 'urgency_min', 'urgency_max', 'urgency_mean', 'urgency_std', 'companyCount_min', 'companyCount_max', 'companyCount_mean', 'companyCount_std', 'sentenceCount_min', 'sentenceCount_max', 'sentenceCount_mean', 'sentenceCount_std', 'relevance_min', 'relevance_max', 'relevance_mean', 'relevance_std', 'sentimentClass_min', 'sentimentClass_max', 'sentimentClass_mean', 'sentimentClass_std', 'sentimentNegative_min', 'sentimentNegative_max', 'sentimentNegative_mean', 'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max', 'sentimentNeutral_mean', 'sentimentNeutral_std', 'sentimentPositive_min', 'sentimentPositive_max', 'sentimentPositive_mean', 'sentimentPositive_std', 'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean', 'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max', 'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min', 'noveltyCount3D_max', 'noveltyCount3D_mean', 'noveltyCount3D_std', 'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean', 'noveltyCount5D_std', 'noveltyCount7D_min', 'noveltyCount7D_max', 'noveltyCount7D_mean', 'noveltyCount7D_std', 'volumeCounts12H_min', 'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std', 'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean', 'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max', 'volumeCounts3D_mean', 'volumeCounts3D_std', 'volumeCounts5D_min', 'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std', 'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean', 'volumeCounts7D_std', 'position_min', 'position_max', 'position_mean', 'position_std', 'coverage_min', 'coverage_max', 'coverage_mean', 'coverage_std']\n\n1 K using data from  20120103  to  20161230\nTraining until validation scores don't improve for 100 rounds.\n[50]\ttraining's binary_logloss: 0.690018\tvalid_1's binary_logloss: 0.691284\n[100]\ttraining's binary_logloss: 0.688003\tvalid_1's binary_logloss: 0.690318\n[150]\ttraining's binary_logloss: 0.686681\tvalid_1's binary_logloss: 0.689861\n[200]\ttraining's binary_logloss: 0.685773\tvalid_1's binary_logloss: 0.689689\n[250]\ttraining's binary_logloss: 0.685134\tvalid_1's binary_logloss: 0.68968\n[300]\ttraining's binary_logloss: 0.684671\tvalid_1's binary_logloss: 0.68975\nEarly stopping, best iteration is:\n[223]\ttraining's binary_logloss: 0.685448\tvalid_1's binary_logloss: 0.689667\n                          feature  split          gain\n100            volumeCounts5D_min      0  0.000000e+00\n22            noveltyCount5D_mean      0  0.000000e+00\n23             noveltyCount5D_min      0  0.000000e+00\n73           sentimentNeutral_max      0  0.000000e+00\n25             noveltyCount7D_max      0  0.000000e+00\n27             noveltyCount7D_min      0  0.000000e+00\n72          sentimentNegative_std      0  0.000000e+00\n33                  relevance_max      0  0.000000e+00\n68             sentimentClass_std      0  0.000000e+00\n67             sentimentClass_min      0  0.000000e+00\n66            sentimentClass_mean      0  0.000000e+00\n65             sentimentClass_max      0  0.000000e+00\n80          sentimentPositive_std      0  0.000000e+00\n93            volumeCounts24H_std      0  0.000000e+00\n92            volumeCounts24H_min      0  0.000000e+00\n82                    urgency_max      0  0.000000e+00\n84                    urgency_min      0  0.000000e+00\n88            volumeCounts12H_min      0  0.000000e+00\n21             noveltyCount5D_max      0  0.000000e+00\n20             noveltyCount3D_std      0  0.000000e+00\n86            volumeCounts12H_max      0  0.000000e+00\n18            noveltyCount3D_mean      0  0.000000e+00\n2                companyCount_min      0  0.000000e+00\n5                   coverage_mean      0  0.000000e+00\n75           sentimentNeutral_min      0  0.000000e+00\n7                    coverage_std      0  0.000000e+00\n19             noveltyCount3D_min      0  0.000000e+00\n10           noveltyCount12H_mean      0  0.000000e+00\n11            noveltyCount12H_min      0  0.000000e+00\n9             noveltyCount12H_max      0  0.000000e+00\n17             noveltyCount3D_max      0  0.000000e+00\n13            noveltyCount24H_max      0  0.000000e+00\n16            noveltyCount24H_std      0  0.000000e+00\n15            noveltyCount24H_min      0  0.000000e+00\n14           noveltyCount24H_mean      0  0.000000e+00\n12            noveltyCount12H_std      0  0.000000e+00\n98             volumeCounts5D_max      1  2.618420e+01\n1               companyCount_mean      1  2.136910e+01\n3                companyCount_std      1  2.821970e+01\n91           volumeCounts24H_mean      1  2.135070e+01\n96             volumeCounts3D_min      1  2.296160e+01\n97             volumeCounts3D_std      1  2.862890e+01\n26            noveltyCount7D_mean      1  2.374940e+01\n74          sentimentNeutral_mean      1  1.875010e+01\n104            volumeCounts7D_min      1  2.346500e+01\n36                  relevance_std      1  2.170950e+01\n34                 relevance_mean      1  2.445450e+01\n28             noveltyCount7D_std      1  2.677170e+01\n32                   position_std      1  1.979910e+01\n69          sentimentNegative_max      1  3.313100e+01\n77          sentimentPositive_max      1  2.341740e+01\n62             sentenceCount_mean      2  5.714340e+01\n76           sentimentNeutral_std      2  4.716130e+01\n70         sentimentNegative_mean      2  4.336880e+01\n78         sentimentPositive_mean      2  4.882310e+01\n94             volumeCounts3D_max      2  5.373290e+01\n35                  relevance_min      2  5.228160e+01\n99            volumeCounts5D_mean      2  5.273270e+01\n95            volumeCounts3D_mean      2  4.613350e+01\n101            volumeCounts5D_std      3  7.122870e+01\n4                    coverage_max      3  7.402380e+01\n6                    coverage_min      3  7.683750e+01\n24             noveltyCount5D_std      3  7.479790e+01\n0                companyCount_max      3  6.349910e+01\n71          sentimentNegative_min      4  9.991650e+01\n85                    urgency_std      4  1.189498e+02\n64              sentenceCount_std      5  1.166999e+02\n83                   urgency_mean      5  1.368023e+02\n30                  position_mean      6  1.575407e+02\n29                   position_max      8  2.033349e+02\n63              sentenceCount_min      8  2.060051e+02\n103           volumeCounts7D_mean      9  2.691407e+02\n90            volumeCounts24H_max     10  3.364421e+02\n102            volumeCounts7D_max     11  2.993439e+02\n87           volumeCounts12H_mean     13  3.689210e+02\n89            volumeCounts12H_std     15  4.893853e+02\n31                   position_min     17  4.901365e+02\n79          sentimentPositive_min     18  4.888805e+02\n61              sentenceCount_max     20  5.269556e+02\n105            volumeCounts7D_std     22  8.033610e+02\n37   returnsCloseOpenMktres10_avg     41  3.944679e+03\n55      returnsOpenPrevMktres_avg     41  1.394866e+03\n59         returnsOpenPrevRaw_avg     44  3.172068e+03\n47     returnsClosePrevMktres_avg     51  3.750551e+03\n51        returnsClosePrevRaw_avg     60  3.117073e+03\n39    returnsCloseOpenMktres1_avg     71  5.759339e+03\n41      returnsCloseOpenRaw10_avg     84  1.514093e+04\n43       returnsCloseOpenRaw1_avg    112  8.915442e+03\n53         returnsOpenPrevMktres1    211  8.581319e+03\n49           returnsClosePrevRaw1    261  1.261247e+04\n60         returnsOpenPrevRaw_sum    276  1.876912e+04\n48     returnsClosePrevMktres_sum    371  2.675920e+04\n45        returnsClosePrevMktres1    380  2.041277e+04\n56      returnsOpenPrevMktres_sum    437  1.999592e+04\n81                    today_trend    456  3.958703e+04\n8                           money    498  2.213995e+04\n52        returnsClosePrevRaw_sum    505  2.319516e+04\n46       returnsClosePrevMktres10    518  3.863795e+04\n50          returnsClosePrevRaw10    819  5.556126e+04\n40    returnsCloseOpenMktres1_sum    848  5.805038e+04\n44       returnsCloseOpenRaw1_sum    878  8.145964e+04\n38   returnsCloseOpenMktres10_sum    917  9.352586e+04\n57            returnsOpenPrevRaw1   1003  7.173056e+04\n42      returnsCloseOpenRaw10_sum   1079  1.761248e+05\n54        returnsOpenPrevMktres10   1423  3.209431e+05\n58           returnsOpenPrevRaw10   1552  1.369822e+06\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "two sigma score :  0.3870538325173211\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "738ef6b4cae41a25b1f5c98ac9d82c202fe90b1f"
      },
      "cell_type": "code",
      "source": "cdf.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b564247ea79a15ff0b555b64b5d53a3a8be57a03"
      },
      "cell_type": "code",
      "source": "\"\"\"\nEarly stopping, best iteration is:\n[223]\ttraining's binary_logloss: 0.685427\tvalid_1's binary_logloss: 0.689652\ntwo sigma score :  0.38887642144890827\ngenerating predictions...\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "109ca8ba15167eaa0d314422cd7563bba838d410"
      },
      "cell_type": "code",
      "source": "\"\"\"\nparams = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n          \"num_leaves\" : 60,\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.005,\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 2018,\n          \"verbosity\" : -1 }\n\nEarly stopping, best iteration is:\n[219]\ttraining's binary_logloss: 0.685482\tvalid_1's binary_logloss: 0.689655\n    two sigma score :  0.3885975760360274\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0818ab149d4d668b6dc56b7d93a19687d83ab7fa"
      },
      "cell_type": "code",
      "source": "##### scaling\ndef post_scaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)/ (std * 8)\n    return np.clip(df,-1,1)\n############################################################\nprint(\"generating predictions...\")\npreddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    cdf = prepare_data(marketdf, newsdf)\n    print(\"predict \", cdf['time'].max())\n    Xp = cdf[features].values\n    preds = model.predict(Xp, num_iteration = model.best_iteration) * 2 - 1\n    predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\n    predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n    env.predict(predtemplatedf)\n    gc.collect()\n\nenv.write_submission_file()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "generating predictions...\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "You can only call `get_prediction_days` once.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dd263868788a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generating predictions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpreddays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmarketdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewsdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredtemplatedf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreddays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarketdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewsdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/kaggle/lib/kaggle/competitions/twosigmanews/env.py\u001b[0m in \u001b[0;36mget_prediction_days\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTwoSigmaNewsEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_var00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var01\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You can only call `get_prediction_days` once.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: You can only call `get_prediction_days` once."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}