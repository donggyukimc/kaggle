{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport pyarrow.parquet as pq\nimport os\n\nimport gc\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(os.listdir('../input'))\n\n# memory reduce function\n# https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndf_train = pd.read_csv(\"../input/metadata_train.csv\")\ndf_train = reduce_mem_usage(df_train)\nprint(\"train : \", df_train.shape)\n\ndf_test = pd.read_csv(\"../input/metadata_test.csv\")\ndf_test = reduce_mem_usage(df_test)\nprint(\"test  : \", df_test.shape)\n\ndf_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7adf448cb8befe30aaf32a5029a66684d662fb36"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.exceptions import UndefinedMetricWarning\n\ndef MCC(tn, fp, fn, tp) :\n    try : \n        return ((tp*tn)-(fp*fn)) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n    except :\n        return 0.0\n\ndef eval_predict(predict, target) :\n    threshold = []\n    mcc = []\n    precision = []\n    recall = []\n    f1 = []\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category = UndefinedMetricWarning)\n        \n        for t in np.arange(0.0, 1.0, 0.001) :\n            tn, fp, fn, tp = confusion_matrix(target == 1, predict > t).ravel()\n            if np.isnan(MCC(tn, fp, fn, tp)) : continue\n            threshold.append(t)\n            mcc.append(MCC(tn, fp, fn, tp))\n            precision.append(precision_score(target == 1, predict > t))\n            recall.append(recall_score(target == 1, predict > t))\n            f1.append(f1_score(target == 1, predict > t) )\n\n    if len(mcc) == 0 :\n        print(\"no valid result!\")\n        return 0.0, 5.0\n    else :\n        best_idx = np.argmax(np.array(mcc))\n        print( 'best result - threshold : ', \"{0:.3f}\".format(threshold[best_idx])\n              ,' MCC : ', \"{0:.2f}\".format(mcc[best_idx])\n              ,' precision : ', \"{0:.2f}\".format(precision[best_idx])\n              ,' recall : ', \"{0:.2f}\".format(recall[best_idx])\n              ,' f1_score : ', \"{0:.2f}\".format(f1[best_idx])\n             )\n        return mcc[best_idx], threshold[best_idx]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "aa05cbde5ee27b95a44d84d09aa06984ee26aa6d"
      },
      "cell_type": "code",
      "source": "from multiprocess import Pool, current_process\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport numba\nimport pywt\nfrom scipy.signal import periodogram\nfrom scipy.signal import find_peaks\nfrom scipy.signal import peak_widths\nfrom tqdm import tqdm\n\n@numba.jit\ndef get_features(x):\n    max_val = x[0]\n    min_val = x[0]\n    for i in x[1:]:\n        if i > max_val:\n            max_val = i\n        elif i < min_val:\n            min_val = i\n    avg_val = np.median(x)\n    #avg_val = np.mean(x)\n    return [\n        abs(min_val - avg_val)\n        , (max_val - avg_val)\n        , np.std(x)\n        , abs(abs(min_val - avg_val) - (max_val - avg_val))\n    ]\n\n@numba.jit\ndef get_stats(x):\n    mean = x[0]\n    maximum = x[0]\n    minimum = x[0]\n    for i in x[1:]:\n        mean += i\n        if i > maximum:\n            maximum = i\n        elif i < minimum:\n            minimum = i\n    mean /= x.shape[0]\n    return [\n          mean,\n          maximum,\n          minimum,\n          np.std(x)\n    ]\n\ndef get_signal(idx, is_train = True, offset = 100, pool = 100) :\n    \n    if is_train :\n        pq_dir = \"../input/train.parquet\"\n    else :\n        pq_dir = \"../input/test.parquet\"\n    idx = np.array(idx)\n    \n    chunk = pq.read_pandas(pq_dir, columns=[str(i) for i in idx]).to_pandas()\n    chunk = chunk.values\n    chunk = chunk.T\n\n    scaler = MinMaxScaler()\n    global_stats = []\n    local_stats = []\n    for data in tqdm(chunk, mininterval = 1) :\n    \n        (ca, cd) = pywt.dwt(data,'haar')\n        cat = pywt.threshold(ca, np.std(ca)/2, 'soft')\n        cdt = pywt.threshold(cd, np.std(cd)/2, 'soft')\n        data = pywt.idwt(cat, cdt, 'haar')\n\n        \"\"\"\n        peaks = find_peaks(data)[0]\n        peak_num = peaks.shape[0] / data.shape[0]\n        width, height, _, _ = peak_widths(data, peaks)\n        width /= data.shape[0]\n        _, den = periodogram(data, 10e3)\n        \n        global_stats.append(\n            [peak_num] + get_stats(width) + get_stats(height) + get_stats(den)\n        )\n        \"\"\"\n    \n        for i in range(0, data.shape[0], offset) :\n            local_stats.append(get_features(data[i : i + pool]))\n            \n    del chunk\n    gc.collect()\n    \n    local_stats = np.array(local_stats, np.float32)\n    local_stats = local_stats.reshape(idx.shape[0], -1, len(local_stats[0]))\n    \n    global_stats = np.array(global_stats, np.float32)\n    \n    return local_stats, global_stats\n\ntrain_x = []\nchunk_size = 1100\nidx = df_train.index.values\ns_idx = 0\ne_idx = s_idx + chunk_size\n\njob_list = []\nwhile s_idx < idx.shape[0] :\n    job_list.append(df_train.loc[idx[s_idx : e_idx], 'signal_id'])\n    s_idx = s_idx + chunk_size\n    e_idx = e_idx + chunk_size\n\npool = Pool(2)\ntrain_x = pool.map(get_signal, job_list)\npool.close()\n\ntrain_signal = [x[0] for x in train_x]\ntrain_meta   = [x[1] for x in train_x]\n\ntrain_signal = np.concatenate(train_signal)\ntrain_meta = np.concatenate(train_meta)\ntrain_y = df_train['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40e1781177c85087f05aa4983be2aa1f33721bf1"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(24, 7))\nfor i in range(train_signal.shape[2]) :\n    plt.plot(train_signal[0, :, i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4aa777de711809c0da6cc092381e8c8c11c2596e"
      },
      "cell_type": "code",
      "source": "m_idx = df_train.groupby('id_measurement').agg({'target' : 'sum'})\nm_idx.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0af2a4f0b707d3e894672c40c55fda0e50b63757"
      },
      "cell_type": "code",
      "source": "import keras.backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Reshape, Dense, Dropout, Bidirectional, LSTM, CuDNNLSTM, Input, concatenate\nfrom keras.layers import BatchNormalization, Activation, AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n\ndef get_callbacks(monitor = 'val_loss', verbose = 1) :\n    callbacks = [\n        EarlyStopping(\n            monitor=monitor\n            , mode='min'\n            , verbose=verbose\n            , restore_best_weights=False\n            , patience=30\n        ) \n        , ModelCheckpoint(\n            filepath='bestModel.md'\n            , monitor=monitor\n            , mode='min'\n            , verbose=0\n            , save_best_only = True\n        )\n    ]\n    return callbacks\n\ndef mcc_f(y_true, y_pred):\n    y_pos_pred = K.round(K.clip(y_pred, 0, 1))\n    y_pos_true = K.round(K.clip(y_true, 0, 1))\n    \n    y_neg_pred = 1 - y_pos_pred\n    y_neg_true = 1 - y_pos_true\n\n    tp = K.sum(y_pos_true * y_pos_pred)\n    tn = K.sum(y_neg_true * y_neg_pred)\n    fp = K.sum(y_neg_true * y_pos_pred)\n    fn = K.sum(y_pos_true * y_neg_pred)\n    return (tp * tn - fp * fn) / (K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) + K.epsilon())\n\ndef get_model(input_shape\n              , lr = 0.001\n              , dropout = 0.0\n              , l2_lambda = 0.0\n              , verbose = False\n              , attention = False\n              , kernel_size = 100\n              , filter_size = 1) :\n\n    inp = Input(shape=input_shape)\n    #inp_meta = Input(shape=(train_meta.shape[1],))\n\n    x = Conv1D(kernel_size = (20)\n                     , filters = 24\n                     , kernel_regularizer = regularizers.l2(l2_lambda))(inp)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = AveragePooling1D(pool_size = (5), strides=(4))(x)\n    \n    x = Conv1D(kernel_size = (20)\n                     , filters = 24\n                     , kernel_regularizer = regularizers.l2(l2_lambda))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = AveragePooling1D(pool_size = (5), strides=(4))(x)\n\n    x = Conv1D(kernel_size = (20)\n                     , filters = 24\n                     , kernel_regularizer = regularizers.l2(l2_lambda))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = AveragePooling1D(pool_size = (5), strides=(4))(x)\n    \n    x = Conv1D(kernel_size = (20)\n                     , filters = 24\n                     , kernel_regularizer = regularizers.l2(l2_lambda))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = AveragePooling1D(pool_size = (5), strides=(4))(x)\n    \n    #x = Bidirectional(CuDNNLSTM(50, return_sequences=True, kernel_regularizer=regularizers.l2(l2_lambda)))(x)\n    x = Bidirectional(CuDNNGRU(50, return_sequences=True, kernel_regularizer=regularizers.l2(l2_lambda)))(x)\n    #x = Dropout(dropout)(x)\n    x = Attention(x.shape[-2])(x)\n    \n    #x = concatenate([inp_meta, x])\n    \n    x = Dense(100, activation = None\n             , kernel_regularizer=regularizers.l2(l2_lambda)\n             )(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout)(x)\n    \n    x = Dense(50, activation = None\n             , kernel_regularizer=regularizers.l2(l2_lambda)\n             )(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout)(x)\n    \n    x = Dense(1, activation = \"sigmoid\"\n             , kernel_regularizer=regularizers.l2(l2_lambda)\n             )(x)\n\n    model = Model(inputs=inp, outputs=x)\n    #model = Model(inputs=[inp, inp_meta], outputs=x)\n    \n    adam = Adam(lr)\n    model.compile(loss='binary_crossentropy', optimizer=adam)\n\n    if verbose :\n        print(model.summary())\n        print(\"lr      : \", lr)\n        print(\"dropout : \", dropout)\n        print(\"l2      : \", l2_lambda)\n        \n    return model",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "784bf615a3e0ae2e1ded60658e35eb2a8dbc9e7b",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import matthews_corrcoef\n\nbatch_size = 128\nepoch = 500\nk = 5\n\ndo_shuffle = True\nrandom_seed = None\n\n#do_shuffle = False\n#random_seed = 2019\n\nnp.random.seed(random_seed)\n\nmodels = []\nthresholds = []\nmccs = []\nvalidate_mccs = []\nvalidate_loss = []\n\ngc.collect()\n\nprint(\"batch size : \", batch_size)\nprint(\"epoch : \", epoch)\nprint(\"k : \", k)\n\n#kfold = StratifiedKFold(k, shuffle = do_shuffle)\n#for i, (train_idx, validate_idx) in enumerate(kfold.split(df_train, df_train['target'])) :\n#kfold = KFold(k, shuffle = do_shuffle)\n#for i, (train_idx, validate_idx) in enumerate(kfold.split(df_train)) :\n\ntrn_idx = []\nval_idx = []\n\nkfold = StratifiedKFold(k, shuffle = do_shuffle, random_state = random_seed)\nfor i, (train_idx, validate_idx) in enumerate(kfold.split(m_idx.index.values, m_idx.target.values > 0)) :\n\n    train_idx = df_train.loc[df_train['id_measurement'].isin(m_idx.index.values[train_idx]), 'signal_id']\n    validate_idx = df_train.loc[df_train['id_measurement'].isin(m_idx.index.values[validate_idx]), 'signal_id']\n    \n    trn_idx.append(train_idx)\n    val_idx.append(validate_idx)\n    \n    model = get_model(train_signal[0].shape\n                      , lr = 0.0002\n                      , dropout = 0.2\n                      , verbose = (i==0)\n                      , attention = True\n                      , filter_size = 5\n                      #, l2_lambda = 0.000001\n                     )\n    \n    print(i + 1, ' fold')\n    \n    model.fit(train_signal[train_idx], train_y[train_idx]\n        #[train_signal[train_idx], train_meta[train_idx]], train_y[train_idx]\n              , epochs = epoch\n              , batch_size = batch_size\n              , validation_data = (train_signal[validate_idx], train_y[validate_idx])\n        #, validation_data = ([train_signal[validate_idx], train_meta[validate_idx]], train_y[validate_idx])\n              , callbacks = get_callbacks()\n             )\n    \n    model.load_weights('bestModel.md')\n    \n    validate_loss.append(model.evaluate(train_signal[validate_idx], train_y[validate_idx], batch_size = batch_size))\n    print(\"loss :\", validate_loss)\n    \n    predict = model.predict(train_signal[validate_idx], batch_size = batch_size)\n    predict = predict.reshape(-1)\n    mcc, threshold = eval_predict(predict, train_y[validate_idx])\n\n    models.append(model)\n    thresholds.append(threshold)\n    mccs.append(mcc)\n    \n    print(\"mcc : \" ,mccs)\n    \n    del model\n    gc.collect()\n    \n    print('')\n    \nprint(\"trained model performances :\")\nprint(\"thresholds - \")\nprint(thresholds) \nprint(\"mcc - \")\nprint(mccs)\nprint(\"avg mcc : \", sum(mccs) / len(mccs))\nprint(\"loss\")\nprint(validate_loss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea7d7b175469f4195901cf2bd7e90a9e751b5658"
      },
      "cell_type": "code",
      "source": "print(\"trained model performances :\")\nprint(\"thresholds - \")\nprint(thresholds) \nprint(\"mcc - \")\nprint(mccs)\nprint(\"avg mcc : \", sum(mccs) / len(mccs))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19e0a0d34a86416027a049448e5f3f89239eb533"
      },
      "cell_type": "code",
      "source": "trained model performances :\nthresholds - \n[0.523, 0.426, 0.34800000000000003, 0.23, 0.127]\nmcc - \n[0.6940941810723269, 0.7297282540363873, 0.7016325710944952, 0.7267056412680362, 0.7114783084132461]\navg mcc :  0.7127277911768984\nloss\n[0.09835298062738713, 0.09363567445275432, 0.09560776766276333, 0.09584901007389243, 0.08889788707782482]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d0733d870ad7cebffca70019806c54786351b145",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "del train_signal\ndel train_meta\ndel train_y\ngc.collect()\n\nchunk_size = 1100\nidx = df_test.index.values\ns_idx = 0\ne_idx = s_idx + chunk_size\npredict = np.zeros(idx.shape[0])\n\nwhile s_idx < idx.shape[0] :\n    print(\"predict \", s_idx, '~', e_idx)\n    \n    job_list = [\n        [df_test.loc[idx[s_idx : int((s_idx + e_idx)/2)], 'signal_id'].values, False]\n        , [df_test.loc[idx[int((s_idx + e_idx)/2) : e_idx], 'signal_id'].values, False]\n               ]\n    job_list = [job for job in job_list if job[0].shape[0] != 0]\n    \n    pool = Pool(2)\n    x = pool.starmap(get_signal, job_list)\n    pool.close()\n    \n    signal = [x_[0] for x_ in x]\n    meta   = [x_[1] for x_ in x]\n\n    signal = np.concatenate(signal)\n    meta = np.concatenate(meta)\n    \n    for i, (model, threshold) in enumerate(zip(models, thresholds)) :\n        #predict[s_idx : e_idx] += model.predict([signal, meta], batch_size = batch_size).reshape(-1) > threshold\n        predict[s_idx : e_idx] += model.predict(signal, batch_size = batch_size).reshape(-1) > threshold\n        #predict[s_idx : e_idx] += model.predict(signal, batch_size = batch_size).reshape(-1)\n    \n    s_idx = s_idx + chunk_size\n    e_idx = e_idx + chunk_size\n    \n    del x\n    del signal\n    del meta\n    gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7f5d0545922f6408b3445894fdf0bab998654e8"
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame()\nsubmission['signal_id'] = df_test['signal_id']\nsubmission['target'] = np.array((predict / k) > validate_threshold, dtype = np.int8)\nsubmission.to_csv('submission_blend.csv', index = False)\n\nprint(submission[submission['target']==1].shape[0] / submission.shape[0])\nsubmission['target'].hist()\nplt.show()\nsubmission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdebd8a8c0bd248319261fe8085888a3570b7fe4",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "for vote in range(1, k + 1) :\n    print(\"result with vote threshold\", vote)\n    \n    predict_vote = (predict >= vote)\n\n    submission = pd.DataFrame()\n    submission['signal_id'] = df_test['signal_id']\n    submission['target'] = np.array(predict_vote, dtype = np.int8)\n    submission.to_csv('submission' + str(vote) + '.csv', index = False)\n    \n    print(submission[submission['target']==1].shape[0] / submission.shape[0])\n    submission['target'].hist()\n    plt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
